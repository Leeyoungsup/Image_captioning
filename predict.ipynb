{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import csv\n",
    "from PIL import Image\n",
    "from caption_model.CNN_RNN import EncoderCNN, DecoderRNN\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Device configurationresul\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':64,\n",
    "        'epochs':10000,\n",
    "        'data_path':'../../data/dataset/coco/',\n",
    "        'train_csv':'train_annotation.csv',\n",
    "        'val_csv':'val_annotation.csv',\n",
    "        'vocab_path':'../../data/dataset/coco/vocab.pkl',\n",
    "        'embed_size':300,\n",
    "        'hidden_size':512,\n",
    "        'num_layers':1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, data_path, csv, class_dataset, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path+class_dataset+'/'\n",
    "        self.df = pd.read_csv(data_path+csv)\n",
    "        self.class_dataset=class_dataset\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        df = self.df\n",
    "        vocab = self.vocab\n",
    "        img_id=df.loc[index]\n",
    "        image_path = self.root+img_id['image_file']\n",
    "        caption=img_id['caption']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def idx2word(vocab, indices):\n",
    "    sentence = []\n",
    "    for i in range(params['batch_size']):\n",
    "        indices[i].cpu().numpy()\n",
    "    \n",
    "    for index in indices:\n",
    "        word = vocab.idx2word[index]\n",
    "        sentence.append(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "with open(params['vocab_path'], 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "transform = transforms.Compose([ \n",
    "        transforms.Resize(params['image_size']),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset=CustomDataset(params['data_path'],params['train_csv'],'train',vocab,transform=transform)\n",
    "val_dataset=CustomDataset(params['data_path'],params['val_csv'],'val',vocab,transform=transform)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)\n",
    "encoder = EncoderCNN(params['embed_size']).to(device)\n",
    "decoder = DecoderRNN(params['embed_size'], params['hidden_size'], len(vocab), params['num_layers']).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_param = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(model_param, lr=params['lr'], betas=(params['beta1'], params['beta2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch._VariableFunctionsClass.rand() argument after * must be an iterable, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membed_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membed_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchinfo/torchinfo.py:220\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchinfo/torchinfo.py:256\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(input_data, input_size, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    254\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[1;32m    255\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[0;32m--> 256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_input_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, correct_input_size\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchinfo/torchinfo.py:529\u001b[0m, in \u001b[0;36mget_input_tensor\u001b[0;34m(input_size, batch_dim, dtypes, device)\u001b[0m\n\u001b[1;32m    527\u001b[0m x \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, dtype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_size, dtypes):\n\u001b[0;32m--> 529\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39mbatch_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: torch._VariableFunctionsClass.rand() argument after * must be an iterable, not int"
     ]
    }
   ],
   "source": [
    "summary(decoder,[(params['batch_size'], params['embed_size']),(params['batch_size'], params['embed_size']),(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6471 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m encoder(images)\n\u001b[1;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m decoder(features, captions, lengths)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(captions\u001b[38;5;241m.\u001b[39mshape())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(lengths)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "train=tqdm(train_dataloader)\n",
    "count=0\n",
    "train_loss = 0.0\n",
    "for images,captions,lengths in train:\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "    features = encoder(images)\n",
    "    outputs = decoder(features, captions, lengths)\n",
    "    print(features.shape())\n",
    "    print(captions.shape())\n",
    "    print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    4,  146,    4,    4,    4,    4,    4,   33,\n",
       "           4,   33,    4,    4,    4,    4,  170,    4,    4,    4,    4,    4,\n",
       "           4,   81,    4,    4,    4,    4,    4,   51,    4,    4,  146,    4,\n",
       "          33,   33,  146,    4,    4,    4,    4,  345,    4,    4,    4,    4,\n",
       "           4,    4,  216,    4, 1361,    4,    4,    4,    4,    4,   48,    4,\n",
       "           4,  194,    4,    4,    4,    4,    4,    4,   99,  131,  336,   13,\n",
       "        1208,   92,  170,  908,  372, 2484, 7380,  113,   31, 5443,   78,  336,\n",
       "          99, 3569,   99,   60,  331,   14, 3213, 3311,  170,   27, 3866,  108,\n",
       "         170,   92,  131,   83,  254, 5661,  131,   92,  331,  336,  135,   40,\n",
       "         170,  335,   92,   60,  408,    3,   11,  336,   94,  331,   21,   20,\n",
       "         336,   60,  324,   21,  844,   52, 2389,  331,   60, 9072,  336,  863,\n",
       "         409,    4,   81,   14, 2078,  738,  580,  271,  327,  791,  291,  259,\n",
       "          72, 2060,    4, 1163,  171,  259,   40, 3268,   14,  780,   14, 5139,\n",
       "         580,  286,   87, 1078,  131,  225,    4, 1690,  170,  131,    4,  327,\n",
       "          14, 1594, 6830,    4,  140,   22,  131, 1534, 1543,   54, 1151,  487,\n",
       "         185,   14,   11,    7,  896,  315,   54,  682,  131,   78,   37,   14,\n",
       "          21,  325, 2725,   29,  378,  170,   14,   51,   22,  162,   78,   55,\n",
       "          78,   31,   14,   78,    4,    4,  821, 8339,   40,  295,    4,   15,\n",
       "         280,  225, 4096,   22,   40,   55, 4551,    4,  140,   59, 2061,   54,\n",
       "         131,  171,   27,  112, 2342,  171,  162,  319, 2299,   51,  852,  278,\n",
       "          78,   32,  924,   14,    4,   52,   22,   21,  860,  158,   32,  538,\n",
       "         757, 4822,  132, 2492, 1616,   40,   78,    9,  361,    7,  269, 2342,\n",
       "        4470, 2356,  122,   33,    4, 3465,    4, 1386,   44,  210,  729,  373,\n",
       "         258,   30, 2339,   22,  132,   78,  171,    4,    4,   48,    7, 1450,\n",
       "           4,    4,   22,  461, 1691,  295,   14,    4,   53,   78, 1193,  701,\n",
       "        1475,  348,  264,  131,  161,    4,   78,  336,  230,   53,   58,  874,\n",
       "          40,   40,    4,   22,  933,  515,   40,   78,  131,    4,    4,   22,\n",
       "           4,   92,  417,  225, 3699,    4,   14,   41,  874,  131,  553, 2094,\n",
       "         123,  940,   55,   65,   14,   31, 1363,   21,   78,   29,  365,  373,\n",
       "         247,  830, 4936,   46,  130,  860,  252,    4,  161,  361, 2170,  133,\n",
       "         225,    4, 7313,   30,  208,    7, 1646,  429, 1694,  265, 1626,    7,\n",
       "         426,  171,    7,   22,    4,    4,  438,  710,  102,    7,    4,    4,\n",
       "          40,  265,  438, 3729, 3800,  140,   78,   40,  177, 1587,    4,   22,\n",
       "         286, 3399,   40,   87,  131,   78, 4096,  131,    4,   22,  162,   86,\n",
       "         122,   22,  162, 3576,  207,  112,   78,   33,   78,   22, 1182,  454,\n",
       "        2138,  161,   57,   80,  509, 1814,   14,   72,  277,    4,   78, 5654,\n",
       "         112,    7, 2196,  196,    4,   40,    4, 2431,  307,  113,   14,   14,\n",
       "          33,   48,  229,  725,    4,  479,   14,  449, 7021,    4,   33,   33,\n",
       "         124,  713,    3, 1004,  978, 1361,    4,  561, 1432,  122,  177, 3246,\n",
       "         514,  525,  370,    7,   14, 1734,    4,  501,    4,    4,    4, 6351,\n",
       "           4,   33,   78,   78,   40,   51,   78,  264,   40,   80,    4,    4,\n",
       "           4,  372,    4,   40,   33,   75, 1316, 1398, 1081,    4,   57, 1648,\n",
       "        1929, 1411,  279, 1402,  279, 1442,   34,  205, 2747,   19,  279,    2,\n",
       "         658, 4700,  666,   56, 1920,   40,  110, 7066,   35, 1444, 2339, 3968,\n",
       "         490,   14,  650,   22,  701,  335,   55,    4,    4,  386,   75, 1233,\n",
       "           5,  373, 2361,  748,  339,  565,   33,   33,  161,  968,  103,  132,\n",
       "          33,  124, 1393, 2244,  756,   40,   29,    4, 1676,  247,   19, 1470,\n",
       "          19,   56,   19,   19,   19,   19,   19,   19,   19,    2,    2,    2,\n",
       "           2,    2,    2,   22, 1216,   40,   22,   87,    4,  908,   78,    4,\n",
       "         282,  986,   87,   14,    4, 3760, 1073,  508,  765,    4,  358,  210,\n",
       "          40, 1081, 2454,  896, 3576,  148,  578,  335,  119,  138,  575,  756,\n",
       "         500,   11,  130,   56,  137,   19,   19,   19,   47,   19, 1814,   19,\n",
       "          19,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          33, 1435,    4,  257, 2782, 1678,   78,   47,    5,  795,  436,    7,\n",
       "           4,   20,   47,    3,  320,   47,  701, 1620,  940,  113,   19,   19,\n",
       "         387,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
       "          19,   19,    2,    2,    2,    2,    2,    2,    2,    2, 8650,   22,\n",
       "        6814, 7842,   87,   35,    4,  409, 1737,   33,    4, 9644,  524,   37,\n",
       "         409, 1075,   19,   19,   19,   19,   19,   19,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "         730,   51,  595,   78, 8843,   33,  229,  326, 1368,   34, 1322,   19,\n",
       "          19,   19, 6600,   19,    2,    2,    2,    2,    2,    2,    7, 3946,\n",
       "         112,   33,    7, 1631,   19,   19,   19,   19,   19,    2,    2,    2,\n",
       "           2,    2,    4,   40,   33,  527, 1498,   19,    2,    2,    2,    2,\n",
       "           2, 3204,   33,   56,   19,   19,    2, 8806,  527,   19,    2,    2,\n",
       "          40,   19,    2, 8650,    2,   19,    2], device='cuda:5')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
